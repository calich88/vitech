{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Mạng Neural\n",
    "===============\n",
    "\n",
    "(Dịch từ bản tiếng Anh: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)\n",
    "\n",
    "Khi sử dụng pytorch, package ``torch.nn`` sẽ giúp bạn xây dựng một mạng neural.\n",
    "\n",
    "Trong bài trước, ta đã tìm hiểu về package ``autograd`` - tính đạo hàm tự động, package ``nn`` sẽ phụ thuộc vào ``autograd`` để định nghĩa mô hình và tính đạo hàm.\n",
    "\n",
    "Để định nghĩa một mô hình mạng Neural, bạn định nghĩa một ``nn.Module`` gồm các lớp của mạng Neural và thủ tục tiến ``forward(input)`` mô tả quá trình tính toán trên dữ liệu vào và trả về kết quả.\n",
    "\n",
    "Để ví dụ, hãy xem xét mạng neural dùng để phân loại chữ số này:\n",
    "\n",
    "![convnet](files/ch3.png)\n",
    "\n",
    "Đây là một mạng neural đơn giản mà dữ liệu chỉ tiến về phía trước (feed-forward): dữ liệu đầu vào (hình ảnh chữ số) được đi qua từng lớp trên mạng, và cuối cùng tính ra kết quả (là xác suất để ảnh là chữ số từ 0 đến 9).\n",
    "\n",
    "Quá trình huấn luyện một mạng neural như vậy thường bao gồm các bước sau:\n",
    "\n",
    "- Định nghĩa mạng neural với các tham số cần học (hay còn gọi là trọng số/weights)\n",
    "- Lặp qua tập hợp các dữ liệu đầu vào\n",
    "    - Cho mạng tính kết quả dựa trên dữ liệu đầu vào\n",
    "    - Tính sai số của kết quả với kết quả mong đợi (còn gọi là hàm mất mát / loss function)\n",
    "    - Tính đạo hàm/gradient (của hàm mất mát) trên các trọng số của mạng (bước này còn gọi là lan truyền ngược / backpropgation bởi vì việc tính đạo hàm sẽ diễn ra bằng cách lần ngược từng lớp của mạng)\n",
    "    - Cập nhật giá trị mới của trọng số của mạng; chẳng hạn ta có thể dùng công thức đơn giản sau: ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Như trong bài trước về ``autograd``, ta thấy sức mạnh của ``autograd`` sẽ thể hiện ở chỗ, bạn chỉ cần định nghĩa mạng Neural với các lớp và hàm ``forward``, việc tính đạo hàm bằng lan truyền ngược (hàm ``backward``) sẽ được định nghĩa tự động bởi ``autograd``.\n",
    "\n",
    "\n",
    "Định nghĩa mạng\n",
    "------------------\n",
    "\n",
    "Giờ ta hãy định nghĩa mạng trên đây bằng pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Đầu vào: 1 channel ảnh, đầu ra: 6 channels, convolution: vuông 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Lớp biến đổi tuyến tính / kết nối hoàn toàn (fully connected): y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max-pool trên cửa sổ kích thước 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # Nếu cửa sổ là hình vuông bạn cũng có thể chỉ ghi kích thước một cạnh\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # Loại trừ đi chiều đầu tiên (kích thước của loạt dữ liệu / batch)\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như đã đề cập ở trên, bạn chỉ cần định nghĩa hàm tiến ``forward``, và hàm lan truyền ngược ``backward``\n",
    "(để tính đạo hàm) sẽ được ``autograd`` tự định nghĩa giúp bạn.\n",
    "Trong hàm ``forward`` bạn có thể sử dụng bất kỳ phép tính nào trên ``Tensor``.\n",
    "\n",
    "Các trọng số cần học của mô hình sẽ được trả về qua ``net.parameters()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # Trọng số của lớp convolution thứ nhất conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giớ ta hãy thử sinh ra một ảnh đầu vào 32x32 ngẫu nhiên.\n",
    "Chú ý: mạng trên đây yêu cầu kích thước ảnh đầu vào là 32x32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0703,  0.0024,  0.1143, -0.1114,  0.0750, -0.0705,  0.0376,  0.0740,\n",
      "          0.0815, -0.0025]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó lan truyền ngược để tính đạo hàm (đầu tiên phải gọi `zero_grad` để làm trống bộ nhớ đệm - bởi vì mặc định pytorch sẽ tích luỹ giá trị mỗi khi `backward` được gọi để tính đạo hàm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0789, -0.0802,  0.0725,  0.0578, -0.0520],\n",
      "          [-0.0380, -0.1953,  0.0825,  0.0065, -0.0445],\n",
      "          [ 0.0083, -0.0156, -0.0230, -0.0029, -0.0643],\n",
      "          [-0.0198, -0.0251,  0.0087,  0.1955, -0.0365],\n",
      "          [-0.0376,  0.0085,  0.0276, -0.0303,  0.0968]]],\n",
      "\n",
      "\n",
      "        [[[-0.0163,  0.0462,  0.0742,  0.0521, -0.1239],\n",
      "          [-0.0291, -0.0697,  0.0558, -0.0039, -0.0251],\n",
      "          [-0.0289,  0.0104,  0.0454,  0.0855,  0.0675],\n",
      "          [ 0.0322, -0.0191,  0.0179,  0.0635, -0.0607],\n",
      "          [-0.0230,  0.1631, -0.0879, -0.0708,  0.0213]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0494,  0.0489,  0.0968, -0.1050, -0.0229],\n",
      "          [ 0.0295,  0.0226, -0.0733, -0.0063,  0.1338],\n",
      "          [ 0.1096, -0.0835,  0.0511, -0.0067,  0.1064],\n",
      "          [ 0.0334, -0.0353, -0.0327, -0.0292, -0.0745],\n",
      "          [ 0.1014,  0.0305, -0.0195,  0.0011, -0.0249]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0913,  0.0086, -0.0248, -0.0151, -0.0624],\n",
      "          [-0.0166, -0.0240, -0.0325, -0.0981, -0.0094],\n",
      "          [ 0.0619,  0.0575,  0.0542,  0.0124, -0.0341],\n",
      "          [ 0.0474, -0.0524,  0.1734, -0.2063, -0.1133],\n",
      "          [ 0.0553,  0.1143, -0.0678, -0.0393, -0.0583]]],\n",
      "\n",
      "\n",
      "        [[[-0.0053, -0.0947, -0.0046,  0.1039,  0.0296],\n",
      "          [-0.0390, -0.0911, -0.0973,  0.0417,  0.0197],\n",
      "          [-0.0348,  0.0431, -0.0402,  0.0474, -0.0553],\n",
      "          [-0.0985,  0.0061, -0.0412, -0.0462,  0.0437],\n",
      "          [ 0.0561,  0.0686,  0.1348, -0.0334, -0.0703]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0082,  0.0789, -0.0239,  0.0212, -0.1642],\n",
      "          [-0.0627,  0.0275,  0.0115, -0.0882, -0.0588],\n",
      "          [-0.1584,  0.0301,  0.0460, -0.0743, -0.0044],\n",
      "          [-0.0848, -0.0549, -0.0139,  0.0180,  0.0095],\n",
      "          [ 0.0630,  0.0755,  0.0475, -0.0262,  0.0521]]]])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))\n",
    "# Giá trị đạo hàm trên trọng số của conv1\n",
    "print(params[0].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Chú ý</h4><p>Package ``torch.nn`` chỉ hỗ trợ xử lý dữ liệu theo loạt (mini-batch), chứ không hỗ trợ một đơn vị dữ liệu đầu vào duy nhất.\n",
    "\n",
    "    Ví dụ, ``nn.Conv2d`` sẽ cần dữ liệu vào là Tensor 4 chiều với định dạng\n",
    "    ``(số ảnh) x (số channel) x (chiều cao) x (chiều rộng)`` (còn gọi là định dạng NCHW - (num samples) x (num channels) x (height) x (width)).\n",
    "\n",
    "    Nếu bạn có một tấm ảnh duy nhất thì chỉ cần dùng ``input.unsqueeze(0)`` để thêm vào một chiều nữa cho số lượng ảnh.</p></div>\n",
    "\n",
    "Trước khi tiếp tục, hãy điểm lại những khái niệm đã sử dụng.\n",
    "\n",
    "**Tóm tắt:**\n",
    "  -  ``torch.Tensor`` - Mảng nhiều chiều có hỗ trợ phép tính đạo hàm tự động như ``backward()``. Đối tượng này cũng chịu trách nhiệm lưu trữ giá trị đạo hàm tính được trên Tensor này.\n",
    "  -  ``nn.Module`` - Thể hiện một mạng Neural, là cách thuận tiện để mô tả các trọng số cần học, với các tiện ích để di chuyển sang GPU, xuất, mở,... \n",
    "  -  ``nn.Parameter`` - Là một loại Tensor được tự động ghi nhận là trọng số cần học khi ta định nghĩa một ``Module``.\n",
    "  -  ``autograd.Function`` - Môt tả hàm tiến ``forward`` và truy ngược ``backward`` trong một phép tính toán hỗ trợ lấy đạo hàm tự động. Mỗi phép toán trên ``Tensor`` sẽ tạo ra ít nhất một đỉnh ``Function`` nối với các hàm tạo ra ``Tensor`` này (trên đồ thị tính toán) và ghi lại lịch sử quá trình tính toán.\n",
    "\n",
    "**Đến đây, ta đã thực hiện được các bước sau của quá trình huấn luyện mạng Neural:**\n",
    "  -  Định nghĩa mạng Neural\n",
    "  -  Tính giá trị tiên đoán với dữ liệu đầu vào và lan truyền ngược để tính đạo hàm\n",
    "\n",
    "**Các bước còn lại:**\n",
    "  -  Tính độ mất mát\n",
    "  -  Cập nhất trọng số của mạng\n",
    "\n",
    "Hàm mất mát\n",
    "-------------\n",
    "Hàm mất mát nhận vào cặp `(giá trị tiên đoán, giá trị mục tiêu)` và tính sai số giữa hai giá trị.\n",
    "\n",
    "Package nn của pytorch hỗ trợ nhiều hàm mất mát khác nhau \n",
    "(chi tiết https://pytorch.org/docs/nn.html#loss-functions).\n",
    "Một hàm mất mát đơn giản là ``nn.MSELoss`` tính sai số toàn phương trung bình (mean-squared error) giữa giá trị tiên đoán và giá trị đích.\n",
    "\n",
    "Ví dụ sau cho tính sai số MSE bằng pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6799, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # Một giá trị mục tiêu được khởi tạo ngẫu nhiên\n",
    "target = target.view(1, -1)  # Làm giá trị mục tiêu có cùng kích thước với giá trị tiên đoán\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đến đây nếu ta truy ngược từ hàm mất mát  ``loss``, dùng thuộc tính\n",
    "``.grad_fn`` để chỉ hàm tính đạo hàm, ta sẽ thấy đồ thị tính toán giống như sau:\n",
    "\n",
    "::\n",
    "\n",
    "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "          -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "Do đó, khi ta gọi ``loss.backward()``,  toàn bộ đồ thị sẽ được tính đạo hàm của hàm mất mát trên các trọng số cần học của mạng, và các Tensors được khai báo với ``requires_grad=True`` sẽ có thuộc tính ``.grad`` được tích luỹ với giá trị đạo hàm.\n",
    "\n",
    "Ta hãy truy ngược vài bước từ hàm `loss` để minh hoạ đồ thị tính toán trên đây:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x11fd53320>\n",
      "<ThAddmmBackward object at 0x11fd53780>\n",
      "<ExpandBackward object at 0x11fd53320>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lan truyền ngược\n",
    "--------\n",
    "Để lan truyền ngược tính đạo hàm của hàm mất mát ta chỉ cần gọi ``loss.backward()``.\n",
    "Lưu ý, đầu tiên ta cần xoá các giá trị đạo hàm hiện tại, bởi vì mặc định giá trị đạo hàm sẽ được tích luỹ .\n",
    "\n",
    "\n",
    "Bây giờ ta thử gọi ``loss.backward()``, và in ra đạo hàm của trọng số `bias` của lớp `conv1` trước và sau lệnh gọi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad trước khi gọi backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad sau khi gọi backward\n",
      "tensor([ 0.0285, -0.0119, -0.0106,  0.0423, -0.0105,  0.0002])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # Xoá giá trị đạo hàm hiện tại\n",
    "\n",
    "print('conv1.bias.grad trước khi gọi backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad sau khi gọi backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đến đây ta đã biết cách sử dụng hàm mất mát.\n",
    "\n",
    "**Đọc thêm:**\n",
    "\n",
    "  Package `nn` của pytorch hỗ trợ nhiều loại modules và hàm mất mát cấu thành những yếu tố cơ bản của mạng Neural sâu. Đọc thêm tài liệu đầy đủ tại đây https://pytorch.org/docs/nn.\n",
    "\n",
    "**Điều còn lại duy nhất:**\n",
    "\n",
    "  - Cập nhật trọng số của mạng\n",
    "\n",
    "Cập nhật trọng số\n",
    "------------------\n",
    "\n",
    "Phép cập nhật trọng số đơn giản nhất là phép xuống dốc ngẫu nhiên (Stochastic Gradient Descent / SGD):\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Ta có thể cài đặt phép cập nhật này bằng đoạn mã đơn giản sau đây:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "Tuy nhiên, khi học về mạng Neural, ta sẽ thấy có nhiều phép cập nhật khác (hiệu quả hơn trong nhiều trường hợp)\n",
    "như SGD, Nesterov-SGD, Adam, RMSProp,... Các phép cập nhật này được hỗ trợ trong package ``torch.optim``. Cách sử dụng đơn giản nhu sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Tạo bộ phận tối ưu hoá\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Cập nhật trọng số trong vòng lặp huấn luyện:\n",
    "optimizer.zero_grad()   # Xoá giá trị đạo hàm hiện tại\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Cập nhật trọng số"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Lưu ý::\n",
    "\n",
    "      Ở đây ta cũng phải xoá giá trị đạo hàm hiện tại bằng cách gọi\n",
    "      ``optimizer.zero_grad()`` bởi vì mặc định giá trị đạo hàm sẽ được tích luỹ như đã giải thích ở trên."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
